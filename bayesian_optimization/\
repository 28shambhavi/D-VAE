import pdb
import pickle
import gzip
import sys
import os
import os.path
import collections
import torch
from tqdm import tqdm
import itertools
from scipy.spatial.distance import pdist
import matplotlib.pyplot as plt
from matplotlib.ticker import MaxNLocator
from sparse_gp import SparseGP
import scipy.stats as sps
import numpy as np
from scipy.io import loadmat
from scipy.stats.stats import pearsonr
sys.path.append('%s/..' % os.path.dirname(os.path.realpath(__file__))) 
sys.path.insert(0, '../')
from util import *

'''
Change experiment settings here
'''
data_type = 'BN'
data_type = 'ENAS'
if data_type == 'ENAS':
    max_n = 8  # number of nodes
    save_appendix0 = 'DVAE14bi'
    save_appendix0 = 'DVAE_GCN14bi'
    save_appendix1 = 'SVAE_oneshot14'
    save_appendix1 = 'DVAE14bi_permute'
    model_name0 = 'D-VAE'
    model_name0 = 'D-VAE-GCN'
    model_name1 = 'S-VAE-oneshot'
    model_name1 = 'D-VAE-permute'
    save_appendix0 = 'DVAE14bitest2'
    save_appendix1 = 'SVAE14test2'
    save_appendix0 = 'DVAE56bi'
    save_appendix1 = 'SVAE56'
    model_name0 = 'D-VAE'
    model_name1 = 'S-VAE'
    
    res_dir = 'backup_2_28/'
    res_dir = 'backup7_56/'
    res_dir = 'backup8_56/'
    #res_dir = 'backup5/'
    res_dir = 'backup9_56/'
    res_dir = 'backup8_56_2/'
    res_dir = 'backup8_56_3/'
    res_dir = 'backup8_56_4/'
    res_dir = 'backup10_14/'
    res_dir = 'backup8_56_randn/'
    n_iter = 10
    
    random_baseline = False  # whether to compare BO with random 
    include_train_scores = False

elif data_type == 'BN':
    max_n = 10  # number of nodes
    save_appendix0 = 'DVAE_BN56uni'
    save_appendix1 = 'SVAE'
    model_name0 = 'D-VAE'
    model_name1 = 'S-VAE'
    random_baseline = True  # whether to compare BO with random 
    include_train_scores = False
    n_iter = 5


aggregate_dir = '{}_aggregate_results_{}_{}_{}/'.format(data_type, res_dir[:-1], save_appendix0, save_appendix1)
if not os.path.exists(aggregate_dir):
    os.makedirs(aggregate_dir) 



All_Scores0 = []
All_Arcs0 = []
All_Scores1 = []
All_Arcs1 = []
All_Random_scores0 = []
All_Random_scores1 = []
All_Train_scores0 = []
All_Train_arcs0 = []
All_Train_scores1 = []
All_Train_arcs1 = []
all_test_rmse0, all_test_ll0, all_test_r0 = [], [], []
all_test_rmse1, all_test_ll1, all_test_r1 = [], [], []
for random_seed in range(1, 11):
    save_dir0 = '{}results_{}_{}/'.format(res_dir, save_appendix0, random_seed)  # where to load the BO results of first model
    save_dir1 = '{}results_{}_{}/'.format(res_dir, save_appendix1, random_seed)  # where to save the BO results of second model
    if random_baseline:
        random_dir0 = '{}results_{}_{}/'.format(res_dir, save_appendix0, random_seed)
        random_dir1 = '{}results_{}_{}/'.format(res_dir, save_appendix1, random_seed)

    mean_y_train0, std_y_train0 = load_object('{}results_{}_{}/mean_std_y_train.dat'.format(res_dir, save_appendix0, random_seed))
    mean_y_train1, std_y_train1 = load_object('{}results_{}_{}/mean_std_y_train.dat'.format(res_dir, save_appendix1, random_seed))

    Train_scores0 = []
    Train_scores1 = []
    Train_arcs0 = []
    Train_arcs1 = []
    Scores0 = []
    Arcs0 = []
    Scores1 = []
    Arcs1 = []
    Random_scores0 = []
    Random_scores1 = []
    pbar = tqdm(range(n_iter))
    for iteration in pbar:
        scores0 = load_object("{}scores{}.dat".format(save_dir0, iteration))
        scores1 = load_object("{}scores{}.dat".format(save_dir1, iteration))
        arcs0 = load_object("{}valid_arcs_final{}.dat".format(save_dir0, iteration))
        arcs1 = load_object("{}valid_arcs_final{}.dat".format(save_dir1, iteration))
        Scores0.append(scores0)
        Scores1.append(scores1)
        Arcs0.append(arcs0)
        Arcs1.append(arcs1)
        if random_baseline:
            random_scores0 = load_object("{}random_scores{}.dat".format(random_dir0, iteration))
            random_scores1 = load_object("{}random_scores{}.dat".format(random_dir1, iteration))
            Random_scores0.append(random_scores0)
            Random_scores1.append(random_scores1)

    if include_train_scores:
        train_scores0 = load_object("{}scores-1.dat".format(save_dir0))
        train_scores1 = load_object("{}scores-1.dat".format(save_dir1))
        train_arcs0 = load_object("{}valid_arcs_final-1.dat".format(save_dir0))
        train_arcs1 = load_object("{}valid_arcs_final-1.dat".format(save_dir1))
        Train_scores0.append(train_scores0)
        Train_scores1.append(train_scores1)
        Train_arcs0.append(train_arcs0)
        Train_arcs1.append(train_arcs1)

    Scores0 = np.array(Scores0) * std_y_train0 + mean_y_train0
    Scores1 = np.array(Scores1) * std_y_train1 + mean_y_train1
    Random_scores0 = np.array(Random_scores0) * std_y_train0 + mean_y_train0
    Random_scores1 = np.array(Random_scores1) * std_y_train1 + mean_y_train1
    Train_Scores0 = np.array(Train_scores0)
    Train_Scores1 = np.array(Train_scores1)

    All_Scores0.append(Scores0)
    All_Scores1.append(Scores1)
    All_Arcs0.append(Arcs0)
    All_Arcs1.append(Arcs1)
    All_Random_scores0.append(Random_scores0)
    All_Random_scores1.append(Random_scores1)
    All_Train_scores0.append(Train_scores0)
    All_Train_scores1.append(Train_scores1)
    All_Train_arcs0.append(Train_arcs0)
    All_Train_arcs1.append(Train_arcs1)

    test_rmse0, test_ll0, test_r0 = [], [], []
    test_rmse1, test_ll1, test_r1 = [], [], []
    with open(save_dir0 + 'Test_RMSE_ll.txt', 'r') as f:
        lines = f.readlines()
        for i, line in enumerate(lines):
            if i >= n_iter:
                break
            blocks = line.split(',')
            rmse, ll, r = blocks[0][-6:], blocks[1][-6:], blocks[2][-6:]
            test_rmse0.append(float(rmse))
            test_ll0.append(float(ll))
            test_r0.append(float(r))
    with open(save_dir1 + 'Test_RMSE_ll.txt', 'r') as f:
        lines = f.readlines()
        for i, line in enumerate(lines):
            if i >= n_iter:
                break
            blocks = line.split(',')
            rmse, ll, r = blocks[0][-6:], blocks[1][-6:], blocks[2][-6:]
            test_rmse1.append(float(rmse))
            test_ll1.append(float(ll))
            test_r1.append(float(r))
    all_test_rmse0.append(test_rmse0)
    all_test_rmse1.append(test_rmse1)
    all_test_ll0.append(test_ll0)
    all_test_ll1.append(test_ll1)
    all_test_r0.append(test_r0)
    all_test_r1.append(test_r1)


All_Scores0 = np.array(All_Scores0)  # n_random_seeds * n_bo_iters * n_bo_batch_selections
All_Scores1 = np.array(All_Scores1)
All_Random_scores0 = np.array(All_Random_scores0)
All_Random_scores1 = np.array(All_Random_scores1)
All_Train_scores0 = np.array(All_Train_scores0)
All_Train_scores1 = np.array(All_Train_scores1) 
all_test_rmse0 = np.array(all_test_rmse0)
all_test_rmse1 = np.array(all_test_rmse1)
all_test_ll0 = np.array(all_test_ll0)
all_test_ll1 = np.array(all_test_ll1)
all_test_r0 = np.array(all_test_r0)
all_test_r1 = np.array(all_test_r1)



# plot average scores
fig = plt.figure()
plt.errorbar(range(1, n_iter+1), -All_Scores0.mean(2).mean(0), All_Scores0.mean(2).std(0), label=model_name0 + '+BO')
plt.errorbar(range(1, n_iter+1), -All_Scores1.mean(2).mean(0), All_Scores1.mean(2).std(0), label=model_name1 + '+BO')
if random_baseline:
    plt.errorbar(range(1, n_iter+1), -All_Random_scores0.mean(2).mean(0), All_Random_scores0.mean(2).std(0), label=model_name0 + '+Random')
    plt.errorbar(range(1, n_iter+1), -All_Random_scores1.mean(2).mean(0), All_Random_scores1.mean(2).std(0), label=model_name1 + '+Random')
plt.xlabel('Iteration')
if data_type == 'ENAS':
    plt.ylabel('Average weight-sharing accuracy of the selected batch')
elif data_type == 'BN':
    plt.ylabel('Average BIC of the selected batch')
plt.subplots_adjust(left=0.15)
plt.legend()
ax = fig.gca()
ax.xaxis.set_major_locator(MaxNLocator(integer=True))
plt.savefig(aggregate_dir + 'Average_scores_plot.pdf')
    
# plot best scores
fig = plt.figure()
plt.errorbar(range(1, n_iter+1), -All_Scores0.min(2).mean(0), All_Scores0.min(2).std(0), label=model_name0 + '+BO')
plt.errorbar(range(1, n_iter+1), -All_Scores1.min(2).mean(0), All_Scores1.min(2).std(0), label=model_name1 + '+BO')
if random_baseline:
    plt.errorbar(range(1, n_iter+1), -All_Random_scores0.min(2).mean(0), All_Random_scores0.min(2).std(0), label=model_name0 + '+Random')
    plt.errorbar(range(1, n_iter+1), -All_Random_scores1.min(2).mean(0), All_Random_scores1.min(2).std(0), label=model_name1 + '+Random')
plt.xlabel('Iteration')
if data_type == 'ENAS':
    plt.ylabel('Highest weight-sharing accuracy of the selected batch')
elif data_type == 'BN':
    plt.ylabel('Highest BIC of the selected batch')
plt.subplots_adjust(left=0.15)
plt.legend()
ax = fig.gca()
ax.xaxis.set_major_locator(MaxNLocator(integer=True))
plt.savefig(aggregate_dir + 'Highest_scores_plot.pdf')


# plot best scores over time
def get_highest_over_time(scores):
    highest_mean = scores.max(2).mean(0)
    highest_std = scores.max(2).std(0)
    highest_so_far = [highest_mean[0]]
    std_so_far = [highest_std[0]]
    for i, x in enumerate(highest_mean):
        if i == 0:
            continue
        if x > highest_so_far[-1]:
            cm, cs = x, highest_std[i]
        else:
            cm, cs = highest_so_far[-1], std_so_far[-1]
        highest_so_far.append(cm)
        std_so_far.append(cs)
    return (highest_so_far, std_so_far)


fig = plt.figure()
plt.errorbar(range(1, n_iter+1), *get_highest_over_time(-All_Scores0), label=model_name0 + '+BO')
plt.errorbar(range(1, n_iter+1), *get_highest_over_time(-All_Scores1), label=model_name1 + '+BO')
if random_baseline:
    plt.errorbar(range(1, n_iter+1), *get_highest_over_time(-All_Random_scores0), label=model_name0 + '+Random')
    plt.errorbar(range(1, n_iter+1), *get_highest_over_time(-All_Random_scores1), label=model_name1 + '+Random')
plt.xlabel('Iteration')
if data_type == 'ENAS':
    plt.ylabel('Highest weight-sharing accuracy over time')
elif data_type == 'BN':
    plt.ylabel('Highest BIC over time')
plt.subplots_adjust(left=0.15)
plt.legend()
ax = fig.gca()
ax.xaxis.set_major_locator(MaxNLocator(integer=True))
plt.savefig(aggregate_dir + 'Over_time_highest_scores_plot.pdf')


# plot test rmse, ll, r
for name, label in zip(['rmse', 'll', 'r'], ['RMSE', 'LL', 'Pearson\'s r']):
    fig = plt.figure()
    plt.errorbar(range(1, n_iter+1), eval('all_test_{}0'.format(name)).mean(0), eval('all_test_{}0'.format(name)).std(0), label=model_name0)
    plt.errorbar(range(1, n_iter+1), eval('all_test_{}1'.format(name)).mean(0), eval('all_test_{}1'.format(name)).std(0), label=model_name1)
    plt.xlabel('Iteration')
    plt.ylabel('Test {}'.format(label))
    plt.legend()
    ax = fig.gca()
    ax.xaxis.set_major_locator(MaxNLocator(integer=True))
    plt.savefig(aggregate_dir + 'Test_{}_plot.pdf'.format(name))

f = open(aggregate_dir + 'output.txt', 'a')
for name, label in zip(['rmse', 'll', 'r'], ['RMSE', 'LL', 'Pearson\'s r']):
    string = 'Model 0, first iter, ', name, eval('all_test_{}0'.format(name)).mean(0)[0], eval('all_test_{}0'.format(name)).std(0)[0]
    print(*string)
    print(*string, file=f)
    string = 'Model 1, first iter, ', name, eval('all_test_{}1'.format(name)).mean(0)[0], eval('all_test_{}1'.format(name)).std(0)[0]
    print(*string)
    print(*string, file=f)

     
# print best arcs
flatten_scores0 = [xxx for x in All_Scores0 for xx in x for xxx in xx]
flatten_arcs0 = [xxx for x in All_Arcs0 for xx in x for xxx in xx]
flatten_scores1 = [xxx for x in All_Scores1 for xx in x for xxx in xx]
flatten_arcs1 = [xxx for x in All_Arcs1 for xx in x for xxx in xx]
if include_train_scores:
    flatten_scores0 += [xxx for x in All_Train_scores0 for xx in x for xxx in xx]
    flatten_arcs0 += [xxx for x in All_Train_arcs0 for xx in x for xxx in xx]
    flatten_scores1 += [xxx for x in All_Train_scores1 for xx in x for xxx in xx]
    flatten_arcs1 += [xxx for x in All_Train_arcs1 for xx in x for xxx in xx]

flatten_random_scores0 = [xxx for x in All_Random_scores0 for xx in x for xxx in xx]
flatten_random_scores1 = [xxx for x in All_Random_scores1 for xx in x for xxx in xx]
k = 15
top_k_idxs0 = np.argsort(flatten_scores0)[:k]
print("Top {} arcs selected by {} are".format(k, model_name0))
print("Top {} arcs selected by {} are".format(k, model_name0), file=f)
for rank, i in enumerate(top_k_idxs0):
    print(flatten_arcs0[i], -flatten_scores0[i])
    print(flatten_arcs0[i], -flatten_scores0[i], file=f)
    if data_type == 'ENAS':
        row, _ = decode_ENAS_to_igraph(flat_ENAS_to_nested(flatten_arcs0[i], max_n-2))
    elif data_type == 'BN':
        row, _ = decode_BN_to_igraph(adjstr_to_BN(flatten_arcs0[i]))
    plot_DAG(row, aggregate_dir, 'Model0_top{}'.format(rank), data_type=data_type)
top_k_idxs1 = np.argsort(flatten_scores1)[:k]
print("Top {} arcs selected by {} are".format(k, model_name1))
print("Top {} arcs selected by {} are".format(k, model_name1), file=f)
for rank, i in enumerate(top_k_idxs1):
    print(flatten_arcs1[i], -flatten_scores1[i])
    print(flatten_arcs1[i], -flatten_scores1[i], file=f)
    if data_type == 'ENAS':
        row, _ = decode_ENAS_to_igraph(flat_ENAS_to_nested(flatten_arcs1[i], max_n-2))
    elif data_type == 'BN':
        row, _ = decode_BN_to_igraph(adjstr_to_BN(flatten_arcs1[i]))
    plot_DAG(row, aggregate_dir, 'Model1_top{}'.format(rank), data_type=data_type)

if random_baseline:
    print("Best random scores selected in each space")
    print("Best random scores selected in each space", file=f)
    top_k_idxs0 = np.argsort(flatten_random_scores0)[:k]
    for i in top_k_idxs0:
        print(-flatten_random_scores0[i])
        print(-flatten_random_scores0[i], file=f)
    top_k_idxs1 = np.argsort(flatten_random_scores1)[:k]
    for i in top_k_idxs1:
        print(-flatten_random_scores1[i])
        print(-flatten_random_scores1[i], file=f)

f.close()

